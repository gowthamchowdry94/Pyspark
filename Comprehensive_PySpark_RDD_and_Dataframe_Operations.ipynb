{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Fundamental data structure of Spark. It's an immutable, fault-tolerant, distributed collection of objects.\n",
        "# Cons RDD - No Built-in Optimization,Performance (Often Slower), No Schema\n",
        "# Pros of Dataframe\n",
        "  # DataFrames leverage Spark's Catalyst Optimizer (for logical and physical plan optimization)\n",
        "       # and Tungsten execution engine (for efficient code generation and memory management).\n",
        "  # Schema Awareness\n",
        "  # High-Level API\n",
        "  # SQL Interoperability\n",
        "  # Memory Efficiency: Tungsten optimizes memory usage by using off-heap memory and columnar storage, reducing garbage collection overhead.\n",
        "  # Support for Diverse Data Sources: Integrates seamlessly with a wide range of structured\n",
        "       # and semi-structured data sources (Parquet, ORC, JSON, CSV, JDBC, Hive tables, etc.)."
      ],
      "metadata": {
        "id": "8cmiCUR8anc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commonality (What RDDs and DataFrames Share)\n",
        "  # 1. Distributed & Immutable: Both are distributed collections of data, meaning they are partitioned across a cluster and processed in parallel.\n",
        "          # Once created, neither can be changed; transformations always result in a new RDD or DataFrame.\n",
        "  # 2. Fault Tolerance: Both are fault-tolerant due to their lineage graph. If a partition is lost, Spark can recompute it from its ancestors.\n",
        "  # 3. Lazy Evaluation: Operations on both RDDs and DataFrames are lazy. They are not executed immediately\n",
        "          # but rather build a logical plan (DAG) that is executed only when an action is called (e.g., show(), count(), collect(), write()).\n",
        "  # 4. In-Memory Processing: Both can leverage in-memory caching to significantly speed up iterative algorithms and repeated data access.\n",
        "  # 5. Underlying Foundation: DataFrames are essentially built on top of RDDs. A DataFrame is internally represented as an RDD of Row objects.\n",
        "          # You can convert a DataFrame to an RDD using .rdd and an RDD to a DataFrame using .toDF().\n",
        "  # 6. Language Support: Both support APIs in Scala, Java, Python, and R.\n",
        "\n",
        "# When to Use Which (General Guidance):\n",
        "  # Use DataFrames (preferred for most cases):\n",
        "      # 1. When working with structured or semi-structured data (most common scenario).\n",
        "      # 2. When you need high performance and automatic optimization.\n",
        "      # 3. When you prefer a SQL-like or tabular API.\n",
        "      # 4. When you need to integrate with Spark SQL or other Spark libraries (MLlib, GraphX, Structured Streaming),\n",
        "          # which largely work with DataFrames.\n",
        "  # Use RDDs (for specific niche cases):\n",
        "      # 1. When working with truly unstructured data where defining a schema is impossible or impractical.\n",
        "      # 2. When you need very low-level transformations and fine-grained control that the DataFrame API doesn't directly offer\n",
        "          # (e.g., highly custom serialization, specific memory layouts). This is becoming increasingly rare as\n",
        "          # DataFrames and Datasets gain more features.\n",
        "      # 3. When interacting with legacy Spark codebases that predominantly use RDDs."
      ],
      "metadata": {
        "id": "cDieMgcHsV_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, TimestampType, ArrayType\n",
        "from pyspark.sql.functions import col,lit,expr,when,avg,sum,count,countDistinct,count, \\\n",
        "  approx_count_distinct,min,max,lower,upper,trim,initcap,concat_ws,substring,length,like,substring,length, \\\n",
        "  regexp_extract,regexp_replace,current_date,date_add,date_sub,datediff,month,year,dayofmonth,to_date,to_timestamp, \\\n",
        "  isnull,isnotnull,isnan,explode,array_contains,size, ceil, floor,sqrt,sqrt,round, row_number, rank, dense_rank, ntile, lead,lag, \\\n",
        "  format_string,format_number,udf,broadcast\n",
        "from pyspark.sql.window import Window\n",
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "6YokufJkqyd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"ComprehensivePySparkExample\"). getOrCreate()"
      ],
      "metadata": {
        "id": "Cy5HZzqzarhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Manual Creating the column name and value and creating the dataframe\n",
        "manual_schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"City\", StringType(), True),\n",
        "    StructField(\"Score\", DoubleType(), True)\n",
        "])\n",
        "manual_data = [\n",
        "    (\"Mike\", 28, \"LA\", 85.5),\n",
        "    (\"Sarah\", 35, \"SF\", 92.0),\n",
        "    (\"John\", 22, \"LA\", 78.0)\n",
        "]\n",
        "df_manual_schema = spark.createDataFrame(manual_data, schema=manual_schema)\n",
        "print(\"\\n--- DataFrame created with Manual Schema (df_manual_schema) ---\")\n",
        "df_manual_schema.printSchema()\n",
        "df_manual_schema.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqm_FS0Npxz0",
        "outputId": "913066d3-381c-4033-964a-39ef1c7143a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- DataFrame created with Manual Schema (df_manual_schema) ---\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- City: string (nullable = true)\n",
            " |-- Score: double (nullable = true)\n",
            "\n",
            "+-----+---+----+-----+\n",
            "| Name|Age|City|Score|\n",
            "+-----+---+----+-----+\n",
            "| Mike| 28|  LA| 85.5|\n",
            "|Sarah| 35|  SF| 92.0|\n",
            "| John| 22|  LA| 78.0|\n",
            "+-----+---+----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_manual_schema.describe(\"Age\", \"Score\").show()\n",
        "df_manual_schema.select(\"Age\", \"Score\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeI4IMlTa58e",
        "outputId": "97795c69-eb4b-42d2-a47a-cd5d12b88843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------------+-----------------+\n",
            "|summary|               Age|            Score|\n",
            "+-------+------------------+-----------------+\n",
            "|  count|                 3|                3|\n",
            "|   mean|28.333333333333332|85.16666666666667|\n",
            "| stddev| 6.506407098647712|7.005949852327901|\n",
            "|    min|                22|             78.0|\n",
            "|    max|                35|             92.0|\n",
            "+-------+------------------+-----------------+\n",
            "\n",
            "+---+-----+\n",
            "|Age|Score|\n",
            "+---+-----+\n",
            "| 28| 85.5|\n",
            "| 35| 92.0|\n",
            "| 22| 78.0|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Manual Creating the CSV file and writing it and creating the dataframe out of it\n",
        "\n",
        "csv_file_path = \"temp_data.csv\"\n",
        "csv_data = \"\"\"id,product,price,quantity\n",
        "1,Laptop,1200.00,5\n",
        "2,Mouse,25.50,20\n",
        "3,Keyboard,75.00,10\n",
        "4,Monitor,300.00,3\n",
        "5,Webcam,50.00,8\n",
        "\"\"\"\n",
        "with open(csv_file_path, \"w\") as f:\n",
        "    f.write(csv_data)\n",
        "\n",
        "products_df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
        "products_df.printSchema()\n",
        "products_df.show()"
      ],
      "metadata": {
        "id": "4hKqR50Yp1uK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8732a6-f7f4-43aa-d697-67267708903c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            " |-- price: double (nullable = true)\n",
            " |-- quantity: integer (nullable = true)\n",
            "\n",
            "+---+--------+------+--------+\n",
            "| id| product| price|quantity|\n",
            "+---+--------+------+--------+\n",
            "|  1|  Laptop|1200.0|       5|\n",
            "|  2|   Mouse|  25.5|      20|\n",
            "|  3|Keyboard|  75.0|      10|\n",
            "|  4| Monitor| 300.0|       3|\n",
            "|  5|  Webcam|  50.0|       8|\n",
            "+---+--------+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_rows = [\n",
        "    (\"Alice\", 30, \"NY\", \"2020-01-15\", 100.50, [\"apple\", \"banana\"]),\n",
        "    (\"Bob\", 45, \"CA\", \"2019-03-20\", 250.75, [\"orange\", \"grape\"]),\n",
        "    (\"Charlie\", 25, \"TX\", \"2021-07-01\", 75.20, [\"apple\", \"kiwi\"]),\n",
        "    (\"David\", 50, \"NY\", \"2018-11-10\", 300.00, [\"banana\"]),\n",
        "    (\"Eve\", None, \"CA\", \"2022-02-28\", 120.00, [\"grape\", \"melon\"]),\n",
        "    (\"Frank\", 35, None, \"2020-05-05\", None, [\"apple\", \"orange\"]),\n",
        "    (\"Grace\", 28, \"TX\", \"2021-09-12\", 90.10, []),\n",
        "    (\"Heidi\", 42, \"NY\", \"2019-08-01\", 180.30, [\"banana\", \"kiwi\", \"apple\"]),\n",
        "    (\"Ivan\", 60, \"CA\", \"2017-06-25\", 400.00, [\"orange\"]),\n",
        "    (\"Judy\", 33, \"TX\", \"2020-12-01\", 150.00, None)\n",
        "]\n",
        "columns = [\"Name\", \"Age\", \"State\", \"EnrollmentDate\", \"Amount\", \"Items\"]\n",
        "df = spark.createDataFrame(data_rows, columns)\n",
        "print(\"\\n--- DataFrame created from Python list (df) with Schema Inference ---\")\n",
        "df.printSchema()\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUjhSDc9Swom",
        "outputId": "580f4ed1-4c74-4bd1-ef3e-fcfe4588661b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- DataFrame created from Python list (df) with Schema Inference ---\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- EnrollmentDate: string (nullable = true)\n",
            " |-- Amount: double (nullable = true)\n",
            " |-- Items: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+-------+----+-----+--------------+------+--------------------+\n",
            "|   Name| Age|State|EnrollmentDate|Amount|               Items|\n",
            "+-------+----+-----+--------------+------+--------------------+\n",
            "|  Alice|  30|   NY|    2020-01-15| 100.5|     [apple, banana]|\n",
            "|    Bob|  45|   CA|    2019-03-20|250.75|     [orange, grape]|\n",
            "|Charlie|  25|   TX|    2021-07-01|  75.2|       [apple, kiwi]|\n",
            "|  David|  50|   NY|    2018-11-10| 300.0|            [banana]|\n",
            "|    Eve|NULL|   CA|    2022-02-28| 120.0|      [grape, melon]|\n",
            "|  Frank|  35| NULL|    2020-05-05|  NULL|     [apple, orange]|\n",
            "|  Grace|  28|   TX|    2021-09-12|  90.1|                  []|\n",
            "|  Heidi|  42|   NY|    2019-08-01| 180.3|[banana, kiwi, ap...|\n",
            "|   Ivan|  60|   CA|    2017-06-25| 400.0|            [orange]|\n",
            "|   Judy|  33|   TX|    2020-12-01| 150.0|                NULL|\n",
            "+-------+----+-----+--------------+------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 different methods of filtering from pyspark.sql.functions import col\n",
        "\n",
        "#one column filter, group by, count example in dataframe\n",
        "#processed_df = df.where(df.State == \"NY\")\n",
        "\n",
        "# 2 column filters, group by, count example in dataframe \"|\" = OR operator\n",
        "#processed_df = df.where((df.State == \"NY\") | (df.State == \"TX\")).groupBy(\"State\").count()\n",
        "\n",
        "# 2 column filters, group by, count example in dataframe col, isin operator and .isNull\n",
        "processed_df = df.where((col(\"State\").isin(\"NY\", \"TX\")) | (col(\"state\").isNull())).groupBy(\"State\").count()\n",
        "processed_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2XfRJg1RhiI",
        "outputId": "30e5ae38-8916-4fa1-e28c-8d75834102f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|State|count|\n",
            "+-----+-----+\n",
            "|   NY|    3|\n",
            "|   TX|    3|\n",
            "| NULL|    1|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating temporaryview and converting it as dataframe\n",
        "# Registering as a temporary view to use SQL\n",
        "processed_df.createOrReplaceTempView(\"nyc_tx_state_counts\")\n",
        "\n",
        "#Running the temp view using spark.sql and saving in dataframe\n",
        "sparksqldf = spark.sql(\"SELECT State, count FROM nyc_tx_state_counts ORDER BY count DESC\")\n",
        "sparksqldf.show()\n",
        "\n",
        "# If you want to do more operations on sparksqldf, you can:\n",
        "sparksqldf.filter(col(\"count\") > 1).show()\n",
        "\n",
        "#Running the temp view using spark.table and saving in dataframe\n",
        "sparktabledf = spark.table(\"nyc_tx_state_counts\")\n",
        "sparktabledf.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6u5NrAdDRr1a",
        "outputId": "ceeb693b-c3bd-4f26-e327-1eecd1e23f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|State|count|\n",
            "+-----+-----+\n",
            "|   NY|    3|\n",
            "|   TX|    3|\n",
            "| NULL|    1|\n",
            "+-----+-----+\n",
            "\n",
            "+-----+-----+\n",
            "|State|count|\n",
            "+-----+-----+\n",
            "|   NY|    3|\n",
            "|   TX|    3|\n",
            "+-----+-----+\n",
            "\n",
            "+-----+-----+\n",
            "|State|count|\n",
            "+-----+-----+\n",
            "|   NY|    3|\n",
            "|   TX|    3|\n",
            "| NULL|    1|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2nEMAHvVfaH",
        "outputId": "ca7926c5-abe3-41ec-adae-cd51d7e650ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+--------------+------+--------------------+\n",
            "|   Name| Age|State|EnrollmentDate|Amount|               Items|\n",
            "+-------+----+-----+--------------+------+--------------------+\n",
            "|  Alice|  30|   NY|    2020-01-15| 100.5|     [apple, banana]|\n",
            "|    Bob|  45|   CA|    2019-03-20|250.75|     [orange, grape]|\n",
            "|Charlie|  25|   TX|    2021-07-01|  75.2|       [apple, kiwi]|\n",
            "|  David|  50|   NY|    2018-11-10| 300.0|            [banana]|\n",
            "|    Eve|NULL|   CA|    2022-02-28| 120.0|      [grape, melon]|\n",
            "|  Frank|  35| NULL|    2020-05-05|  NULL|     [apple, orange]|\n",
            "|  Grace|  28|   TX|    2021-09-12|  90.1|                  []|\n",
            "|  Heidi|  42|   NY|    2019-08-01| 180.3|[banana, kiwi, ap...|\n",
            "|   Ivan|  60|   CA|    2017-06-25| 400.0|            [orange]|\n",
            "|   Judy|  33|   TX|    2020-12-01| 150.0|                NULL|\n",
            "+-------+----+-----+--------------+------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Type Changing\n",
        "#using withColumn('column change name', col(source column), change type)\n",
        "\n",
        "df_casted = df.withColumn(\"Age_Double\", col(\"Age\").cast(DoubleType())) \\\n",
        "              .withColumn(\"EnrollmentDate_Date\", col(\"EnrollmentDate\").cast(DateType()))\n",
        "\n",
        "df_casted.select(\"Name\", \"Age\", \"Age_Double\", \"EnrollmentDate\", \"EnrollmentDate_Date\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCOB21n4bPmN",
        "outputId": "36b4f7c0-4eaa-4d2c-fca3-4fab1b13dd16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+----------+--------------+-------------------+\n",
            "|   Name| Age|Age_Double|EnrollmentDate|EnrollmentDate_Date|\n",
            "+-------+----+----------+--------------+-------------------+\n",
            "|  Alice|  30|      30.0|    2020-01-15|         2020-01-15|\n",
            "|    Bob|  45|      45.0|    2019-03-20|         2019-03-20|\n",
            "|Charlie|  25|      25.0|    2021-07-01|         2021-07-01|\n",
            "|  David|  50|      50.0|    2018-11-10|         2018-11-10|\n",
            "|    Eve|NULL|      NULL|    2022-02-28|         2022-02-28|\n",
            "|  Frank|  35|      35.0|    2020-05-05|         2020-05-05|\n",
            "|  Grace|  28|      28.0|    2021-09-12|         2021-09-12|\n",
            "|  Heidi|  42|      42.0|    2019-08-01|         2019-08-01|\n",
            "|   Ivan|  60|      60.0|    2017-06-25|         2017-06-25|\n",
            "|   Judy|  33|      33.0|    2020-12-01|         2020-12-01|\n",
            "+-------+----+----------+--------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding 3 new columns\n",
        "#col(columnname) we are give \"column name\" and column value simply passing through  lit(\"columnvalue\")\n",
        "#new Column name, when (condition passing, value 1).otherwise equal to else statemt\n",
        "\n",
        "df_transformation = df.withColumn(\"Status\", lit(\"Active\",)) \\\n",
        "    .withColumn(\"Status1\", lit(\"Active\",)) \\\n",
        "    .withColumn(\"Status2_AmountTier\",when(col(\"Amount\") > 200, \"High Amount\") \\\n",
        "    .when((col(\"Amount\") >= 100) & (col(\"Amount\") <= 200), \"Medium Amount\") \\\n",
        "    .otherwise(\"Low Amount\")\n",
        "    )\n",
        "\n",
        "df_transformation = df_transformation.withColumn(\"Amount1\", col(\"Amount\") + 1)\n",
        "\n",
        "# Dropping 2 Column\n",
        "df_transformation = df_transformation.drop(\"Status1\",\"Amount1\")\n",
        "\n",
        "# Where and Filter keywords do the same operation\n",
        "\n",
        "#ny_df_transformation = df_transformation.where(df_transformation.State == \"NY\")\n",
        "ny_df_transformation = df_transformation.filter(df_transformation.State == \"NY\")\n",
        "ny_df_transformation.show()\n",
        "\n",
        "# Distinct States\n",
        "df_distinct_states = df_transformation.select(\"State\").distinct()\n",
        "df_distinct_states.show()\n",
        "\n",
        "# Order by (orderBy, sort) (Transformation)\n",
        "df_ordered = ny_df_transformation.orderBy(col(\"Age\").desc(), col(\"Name\").asc())\n",
        "df_ordered.show()\n",
        "\n",
        "# Group by (Transformation)\n",
        "df_transformation.groupBy(\"State\").agg(avg(\"Age\").alias(\"AverageAge\"),sum(\"Amount\").alias(\"TotalAmount\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FostsBgYb_V3",
        "outputId": "9d25234b-b6a3-40d2-8117-22b07351edc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+-----+--------------+------+--------------------+------+------------------+\n",
            "| Name|Age|State|EnrollmentDate|Amount|               Items|Status|Status2_AmountTier|\n",
            "+-----+---+-----+--------------+------+--------------------+------+------------------+\n",
            "|Alice| 30|   NY|    2020-01-15| 100.5|     [apple, banana]|Active|     Medium Amount|\n",
            "|David| 50|   NY|    2018-11-10| 300.0|            [banana]|Active|       High Amount|\n",
            "|Heidi| 42|   NY|    2019-08-01| 180.3|[banana, kiwi, ap...|Active|     Medium Amount|\n",
            "+-----+---+-----+--------------+------+--------------------+------+------------------+\n",
            "\n",
            "+-----+\n",
            "|State|\n",
            "+-----+\n",
            "|   CA|\n",
            "|   NY|\n",
            "|   TX|\n",
            "| NULL|\n",
            "+-----+\n",
            "\n",
            "+-----+---+-----+--------------+------+--------------------+------+------------------+\n",
            "| Name|Age|State|EnrollmentDate|Amount|               Items|Status|Status2_AmountTier|\n",
            "+-----+---+-----+--------------+------+--------------------+------+------------------+\n",
            "|David| 50|   NY|    2018-11-10| 300.0|            [banana]|Active|       High Amount|\n",
            "|Heidi| 42|   NY|    2019-08-01| 180.3|[banana, kiwi, ap...|Active|     Medium Amount|\n",
            "|Alice| 30|   NY|    2020-01-15| 100.5|     [apple, banana]|Active|     Medium Amount|\n",
            "+-----+---+-----+--------------+------+--------------------+------+------------------+\n",
            "\n",
            "+-----+------------------+-----------+\n",
            "|State|        AverageAge|TotalAmount|\n",
            "+-----+------------------+-----------+\n",
            "|   CA|              52.5|     770.75|\n",
            "|   NY|40.666666666666664|      580.8|\n",
            "|   TX|28.666666666666668|      315.3|\n",
            "| NULL|              35.0|       NULL|\n",
            "+-----+------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_expressions = df_transformation.select(\n",
        "    col(\"Name\"),\n",
        "    col(\"Age\"),\n",
        "    when(col(\"Age\") < 30, \"Young\")\n",
        "    .when(col(\"Age\") >= 30, \"Adult\")\n",
        "    .otherwise(\"Senior\").alias(\"AgeGroup_When\"),\n",
        "    expr(\"Age * 1.1\").alias(\"AgeExpr\")\n",
        ")\n",
        "df_expressions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lslP1Ih1cWmP",
        "outputId": "16e52048-1439-4d9f-a44e-81030ebcc68e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-------------+-------+\n",
            "|   Name| Age|AgeGroup_When|AgeExpr|\n",
            "+-------+----+-------------+-------+\n",
            "|  Alice|  30|        Adult|   33.0|\n",
            "|    Bob|  45|        Adult|   49.5|\n",
            "|Charlie|  25|        Young|   27.5|\n",
            "|  David|  50|        Adult|   55.0|\n",
            "|    Eve|NULL|       Senior|   NULL|\n",
            "|  Frank|  35|        Adult|   38.5|\n",
            "|  Grace|  28|        Young|   30.8|\n",
            "|  Heidi|  42|        Adult|   46.2|\n",
            "|   Ivan|  60|        Adult|   66.0|\n",
            "|   Judy|  33|        Adult|   36.3|\n",
            "+-------+----+-------------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create another DataFrame for joins\n",
        "orders_data = [\n",
        "    (\"Alice\", \"Order_A1\", 150),\n",
        "    (\"Bob\", \"Order_B1\", 200),\n",
        "    (\"Charlie\", \"Order_C1\", 50),\n",
        "    (\"Alice\", \"Order_A2\", 75),\n",
        "    (\"David\", \"Order_D1\", 100),\n",
        "    (\"Frank\", \"Order_F1\", 300),\n",
        "    (\"Zara\", \"Order_Z1\", 500) # Zara is not in df\n",
        "]\n",
        "orders_columns = [\"CustomerName\", \"OrderId\", \"OrderValue\"]\n",
        "orders_df = spark.createDataFrame(orders_data, orders_columns)\n",
        "orders_df.show(5)\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYJR_uE4iqlm",
        "outputId": "704e03d7-de69-40d2-f149-5cc06a8eab31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------+----------+\n",
            "|CustomerName| OrderId|OrderValue|\n",
            "+------------+--------+----------+\n",
            "|       Alice|Order_A1|       150|\n",
            "|         Bob|Order_B1|       200|\n",
            "|     Charlie|Order_C1|        50|\n",
            "|       Alice|Order_A2|        75|\n",
            "|       David|Order_D1|       100|\n",
            "+------------+--------+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+----+-----+--------------+------+---------------+\n",
            "|   Name| Age|State|EnrollmentDate|Amount|          Items|\n",
            "+-------+----+-----+--------------+------+---------------+\n",
            "|  Alice|  30|   NY|    2020-01-15| 100.5|[apple, banana]|\n",
            "|    Bob|  45|   CA|    2019-03-20|250.75|[orange, grape]|\n",
            "|Charlie|  25|   TX|    2021-07-01|  75.2|  [apple, kiwi]|\n",
            "|  David|  50|   NY|    2018-11-10| 300.0|       [banana]|\n",
            "|    Eve|NULL|   CA|    2022-02-28| 120.0| [grape, melon]|\n",
            "+-------+----+-----+--------------+------+---------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inner Join: Returns rows when there is a match in both DataFrames.\n",
        "inner_join_df = df.join(orders_df, df[\"Name\"] == orders_df[\"CustomerName\"], \"inner\")\n",
        "#inner_join_df.show()\n",
        "\n",
        "left_outer_join_df = df.join(orders_df, df[\"Name\"] == orders_df[\"CustomerName\"], \"left_outer\")\n",
        "#left_outer_join_df.show()\n",
        "\n",
        "right_outer_join_df = df.join(orders_df, df[\"Name\"] == orders_df[\"CustomerName\"], \"right_outer\")\n",
        "#right_outer_join_df.show()\n",
        "\n",
        "full_outer_join_df = df.join(orders_df, df[\"Name\"] == orders_df[\"CustomerName\"], \"full_outer\")\n",
        "#full_outer_join_df.show()\n",
        "\n",
        "#Returns only left table and also returns only right matching column values\n",
        "left_semi_join_df = df.join(orders_df, df[\"Name\"] == orders_df[\"CustomerName\"], \"left_semi\")\n",
        "#left_semi_join_df.show()\n",
        "\n",
        "#Returns only left table and also returns only not right matching column values\n",
        "left_anti_join_df = df.join(orders_df, df[\"Name\"] == orders_df[\"CustomerName\"], \"left_anti\")\n",
        "#left_anti_join_df.show()"
      ],
      "metadata": {
        "id": "0pE0ImPFpQ8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#---Aggregate Functions ---\n",
        "df.agg(\n",
        "    count(\"*\").alias(\"TotalRecords\"),\n",
        "    count(\"Age\").alias(\"NonNullAges\"), # count non-null values\n",
        "    sum(\"Amount\").alias(\"TotalAmount\"),\n",
        "    avg(\"Amount\").alias(\"AverageAmount\"),\n",
        "    min(\"Age\").alias(\"MinAge\"),\n",
        "    max(\"Age\").alias(\"MaxAge\"),\n",
        "    countDistinct(\"State\").alias(\"DistinctStates\"), # Count distinct values\n",
        "    approx_count_distinct(\"State\").alias(\"ApproxDistinctStates\") # Approximate distinct count (faster for large datasets)\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhJoPPygpydg",
        "outputId": "5bb88613-28d5-4f83-c9d2-475c3d85c63b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----------+-----------+------------------+------+------+--------------+--------------------+\n",
            "|TotalRecords|NonNullAges|TotalAmount|     AverageAmount|MinAge|MaxAge|DistinctStates|ApproxDistinctStates|\n",
            "+------------+-----------+-----------+------------------+------+------+--------------+--------------------+\n",
            "|          10|          9|    1666.85|185.20555555555555|    25|    60|             3|                   3|\n",
            "+------------+-----------+-----------+------------------+------+------+--------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# String Functions ---\n",
        "string_df = df.select(\n",
        "    col(\"Name\"),\n",
        "    lower(col(\"Name\")).alias(\"Name_Lower\"),\n",
        "    upper(col(\"Name\")).alias(\"Name_Upper\"),\n",
        "    trim(lit(\"  Trim Me  \")).alias(\"Trimmed_String\"),\n",
        "    concat_ws(\"-\", col(\"Name\"), col(\"State\")).alias(\"Name_State_Concat\"),\n",
        "    substring(col(\"Name\"), 1, 3).alias(\"Name_Substr\"), # 1-based index\n",
        "    length(col(\"Name\")).alias(\"Name_Length\"),\n",
        "    col(\"Name\").like(\"A%\").alias(\"Name_Starts_With_A_Like\") # Using like for pattern matching\n",
        ")\n",
        "string_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kJWD_f_qTR0",
        "outputId": "1a3365a9-68a4-460b-acf6-4e2acbf91c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+----------+--------------+-----------------+-----------+-----------+-----------------------+\n",
            "|   Name|Name_Lower|Name_Upper|Trimmed_String|Name_State_Concat|Name_Substr|Name_Length|Name_Starts_With_A_Like|\n",
            "+-------+----------+----------+--------------+-----------------+-----------+-----------+-----------------------+\n",
            "|  Alice|     alice|     ALICE|       Trim Me|         Alice-NY|        Ali|          5|                   true|\n",
            "|    Bob|       bob|       BOB|       Trim Me|           Bob-CA|        Bob|          3|                  false|\n",
            "|Charlie|   charlie|   CHARLIE|       Trim Me|       Charlie-TX|        Cha|          7|                  false|\n",
            "|  David|     david|     DAVID|       Trim Me|         David-NY|        Dav|          5|                  false|\n",
            "|    Eve|       eve|       EVE|       Trim Me|           Eve-CA|        Eve|          3|                  false|\n",
            "|  Frank|     frank|     FRANK|       Trim Me|            Frank|        Fra|          5|                  false|\n",
            "|  Grace|     grace|     GRACE|       Trim Me|         Grace-TX|        Gra|          5|                  false|\n",
            "|  Heidi|     heidi|     HEIDI|       Trim Me|         Heidi-NY|        Hei|          5|                  false|\n",
            "|   Ivan|      ivan|      IVAN|       Trim Me|          Ivan-CA|        Iva|          4|                  false|\n",
            "|   Judy|      judy|      JUDY|       Trim Me|          Judy-TX|        Jud|          4|                  false|\n",
            "+-------+----------+----------+--------------+-----------------+-----------+-----------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Regex Functions ---\n",
        "print(\"\\n--- Regex Functions ---\")\n",
        "regex_df = df.withColumn(\"Email\", lit(\"user@example.com\")) \\\n",
        "             .withColumn(\"Phone\", lit(\"123-456-7890\"))\n",
        "\n",
        "regex_df_processed = regex_df.select(\n",
        "    col(\"Name\"),\n",
        "    col(\"Email\"),\n",
        "\n",
        "    #.+ means \"match one or more of any character.\"\n",
        "    regexp_extract(col(\"Email\"), r\"(.+)@\", 1).alias(\"User\"),      # Extracts everything before @\n",
        "    #\\w+ means \"fetching words after the @. + incrmenting \\start and end\n",
        "    regexp_extract(col(\"Email\"), r\"@(\\w+)\\.com\", 1).alias(\"Domain\"), # Extract domain\n",
        "    col(\"Phone\"),\n",
        "    regexp_replace(col(\"Phone\"), r\"-\", \"\").alias(\"Phone_No_Dashes\") # Remove dashes\n",
        ")\n",
        "regex_df_processed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzQyqhyTupVk",
        "outputId": "0e8338e8-b3f0-427f-f18f-d9cabdfe1867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Regex Functions ---\n",
            "+-------+----------------+----+-------+------------+---------------+\n",
            "|   Name|           Email|User| Domain|       Phone|Phone_No_Dashes|\n",
            "+-------+----------------+----+-------+------------+---------------+\n",
            "|  Alice|user@example.com|user|example|123-456-7890|     1234567890|\n",
            "|    Bob|user@example.com|user|example|123-456-7890|     1234567890|\n",
            "|Charlie|user@example.com|user|example|123-456-7890|     1234567890|\n",
            "|  David|user@example.com|user|example|123-456-7890|     1234567890|\n",
            "|    Eve|user@example.com|user|example|123-456-7890|     1234567890|\n",
            "|  Frank|user@example.com|user|example|123-456-7890|     1234567890|\n",
            "|  Grace|user@example.com|user|example|123-456-7890|     1234567890|\n",
            "|  Heidi|user@example.com|user|example|123-456-7890|     1234567890|\n",
            "|   Ivan|user@example.com|user|example|123-456-7890|     1234567890|\n",
            "|   Judy|user@example.com|user|example|123-456-7890|     1234567890|\n",
            "+-------+----------------+----+-------+------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Date Functions ---\n",
        "date_df = df.select(\n",
        "    col(\"Name\"),\n",
        "    col(\"EnrollmentDate\").cast(DateType()).alias(\"EnrollmentDate_Date\"),\n",
        "    current_date().alias(\"CurrentDate\"),\n",
        "    date_add(col(\"EnrollmentDate\"), 7).alias(\"Date_Plus_7_Days\"),\n",
        "    date_sub(col(\"EnrollmentDate\"), 3).alias(\"Date_Minus_3_Days\"),\n",
        "    datediff(current_date(), col(\"EnrollmentDate\")).alias(\"DaysSinceEnrollment\"),\n",
        "    month(col(\"EnrollmentDate\")).alias(\"EnrollmentMonth\"),\n",
        "    year(col(\"EnrollmentDate\")).alias(\"EnrollmentYear\"),\n",
        "    dayofmonth(col(\"EnrollmentDate\")).alias(\"EnrollmentDay\"),\n",
        "    to_date(lit(\"2023-10-26\"), \"yyyy-MM-dd\").alias(\"ParsedDate\"),\n",
        "    to_timestamp(lit(\"2023-10-26 14:30:00\"), \"yyyy-MM-dd HH:mm:ss\").alias(\"ParsedTimestamp\")\n",
        ")\n",
        "date_df.show()\n",
        "date_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4GJhGsnvPis",
        "outputId": "54bb7908-b6fa-41cf-df38-c62167779b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------+-----------+----------------+-----------------+-------------------+---------------+--------------+-------------+----------+-------------------+\n",
            "|   Name|EnrollmentDate_Date|CurrentDate|Date_Plus_7_Days|Date_Minus_3_Days|DaysSinceEnrollment|EnrollmentMonth|EnrollmentYear|EnrollmentDay|ParsedDate|    ParsedTimestamp|\n",
            "+-------+-------------------+-----------+----------------+-----------------+-------------------+---------------+--------------+-------------+----------+-------------------+\n",
            "|  Alice|         2020-01-15| 2025-07-29|      2020-01-22|       2020-01-12|               2022|              1|          2020|           15|2023-10-26|2023-10-26 14:30:00|\n",
            "|    Bob|         2019-03-20| 2025-07-29|      2019-03-27|       2019-03-17|               2323|              3|          2019|           20|2023-10-26|2023-10-26 14:30:00|\n",
            "|Charlie|         2021-07-01| 2025-07-29|      2021-07-08|       2021-06-28|               1489|              7|          2021|            1|2023-10-26|2023-10-26 14:30:00|\n",
            "|  David|         2018-11-10| 2025-07-29|      2018-11-17|       2018-11-07|               2453|             11|          2018|           10|2023-10-26|2023-10-26 14:30:00|\n",
            "|    Eve|         2022-02-28| 2025-07-29|      2022-03-07|       2022-02-25|               1247|              2|          2022|           28|2023-10-26|2023-10-26 14:30:00|\n",
            "|  Frank|         2020-05-05| 2025-07-29|      2020-05-12|       2020-05-02|               1911|              5|          2020|            5|2023-10-26|2023-10-26 14:30:00|\n",
            "|  Grace|         2021-09-12| 2025-07-29|      2021-09-19|       2021-09-09|               1416|              9|          2021|           12|2023-10-26|2023-10-26 14:30:00|\n",
            "|  Heidi|         2019-08-01| 2025-07-29|      2019-08-08|       2019-07-29|               2189|              8|          2019|            1|2023-10-26|2023-10-26 14:30:00|\n",
            "|   Ivan|         2017-06-25| 2025-07-29|      2017-07-02|       2017-06-22|               2956|              6|          2017|           25|2023-10-26|2023-10-26 14:30:00|\n",
            "|   Judy|         2020-12-01| 2025-07-29|      2020-12-08|       2020-11-28|               1701|             12|          2020|            1|2023-10-26|2023-10-26 14:30:00|\n",
            "+-------+-------------------+-----------+----------------+-----------------+-------------------+---------------+--------------+-------------+----------+-------------------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- EnrollmentDate_Date: date (nullable = true)\n",
            " |-- CurrentDate: date (nullable = false)\n",
            " |-- Date_Plus_7_Days: date (nullable = true)\n",
            " |-- Date_Minus_3_Days: date (nullable = true)\n",
            " |-- DaysSinceEnrollment: integer (nullable = true)\n",
            " |-- EnrollmentMonth: integer (nullable = true)\n",
            " |-- EnrollmentYear: integer (nullable = true)\n",
            " |-- EnrollmentDay: integer (nullable = true)\n",
            " |-- ParsedDate: date (nullable = true)\n",
            " |-- ParsedTimestamp: timestamp (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------Null Handling (na, isnull, isnotnull, isnan) ---\n",
        "\n",
        "# Replacing null value withthe 0 and unknown\n",
        "df_filled = df.na.fill({\"Age\": 0, \"State\": \"Unknown\", \"Amount\": 0.0})\n",
        "df_filled.show()\n",
        "\n",
        "# Drop rows with any null values (dropna)\n",
        "df_dropped_any_null = df.na.drop()\n",
        "df_dropped_any_null.show()\n",
        "\n",
        "# Drop rows with nulls in specific columns\n",
        "df_dropped_age_state_null = df.na.drop(subset=[\"Age\", \"State\"])\n",
        "df_dropped_age_state_null.show()\n",
        "\n",
        "# Replace specific values (e.g., replace 0 in Age with 1)\n",
        "df_replaced = df_filled.na.replace(0, 1, \"Age\")\n",
        "df_replaced.show()\n",
        "\n",
        "# Check for null/NaN values using functions (isnull, isnotnull, isnan)\n",
        "df_null_check = df.select(\n",
        "    col(\"Name\"),\n",
        "    isnull(col(\"Age\")).alias(\"IsAgeNull\"),\n",
        "    isnotnull(col(\"Age\")).alias(\"IsAgeNotNull\"),\n",
        "    isnull(col(\"State\")).alias(\"IsStateNull\"),\n",
        "    isnull(col(\"Amount\")).alias(\"IsAmountNull\"),\n",
        "    isnan(col(\"Amount\")).alias(\"IsAmountNaN\") # isnan only works for numeric types\n",
        ")\n",
        "df_null_check.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDKJHWtIzJYa",
        "outputId": "61861f6a-3475-4512-c0d0-3a1832167f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-------+--------------+------+--------------------+\n",
            "|   Name|Age|  State|EnrollmentDate|Amount|               Items|\n",
            "+-------+---+-------+--------------+------+--------------------+\n",
            "|  Alice| 30|     NY|    2020-01-15| 100.5|     [apple, banana]|\n",
            "|    Bob| 45|     CA|    2019-03-20|250.75|     [orange, grape]|\n",
            "|Charlie| 25|     TX|    2021-07-01|  75.2|       [apple, kiwi]|\n",
            "|  David| 50|     NY|    2018-11-10| 300.0|            [banana]|\n",
            "|    Eve|  0|     CA|    2022-02-28| 120.0|      [grape, melon]|\n",
            "|  Frank| 35|Unknown|    2020-05-05|   0.0|     [apple, orange]|\n",
            "|  Grace| 28|     TX|    2021-09-12|  90.1|                  []|\n",
            "|  Heidi| 42|     NY|    2019-08-01| 180.3|[banana, kiwi, ap...|\n",
            "|   Ivan| 60|     CA|    2017-06-25| 400.0|            [orange]|\n",
            "|   Judy| 33|     TX|    2020-12-01| 150.0|                NULL|\n",
            "+-------+---+-------+--------------+------+--------------------+\n",
            "\n",
            "+-------+---+-----+--------------+------+--------------------+\n",
            "|   Name|Age|State|EnrollmentDate|Amount|               Items|\n",
            "+-------+---+-----+--------------+------+--------------------+\n",
            "|  Alice| 30|   NY|    2020-01-15| 100.5|     [apple, banana]|\n",
            "|    Bob| 45|   CA|    2019-03-20|250.75|     [orange, grape]|\n",
            "|Charlie| 25|   TX|    2021-07-01|  75.2|       [apple, kiwi]|\n",
            "|  David| 50|   NY|    2018-11-10| 300.0|            [banana]|\n",
            "|  Grace| 28|   TX|    2021-09-12|  90.1|                  []|\n",
            "|  Heidi| 42|   NY|    2019-08-01| 180.3|[banana, kiwi, ap...|\n",
            "|   Ivan| 60|   CA|    2017-06-25| 400.0|            [orange]|\n",
            "+-------+---+-----+--------------+------+--------------------+\n",
            "\n",
            "+-------+---+-----+--------------+------+--------------------+\n",
            "|   Name|Age|State|EnrollmentDate|Amount|               Items|\n",
            "+-------+---+-----+--------------+------+--------------------+\n",
            "|  Alice| 30|   NY|    2020-01-15| 100.5|     [apple, banana]|\n",
            "|    Bob| 45|   CA|    2019-03-20|250.75|     [orange, grape]|\n",
            "|Charlie| 25|   TX|    2021-07-01|  75.2|       [apple, kiwi]|\n",
            "|  David| 50|   NY|    2018-11-10| 300.0|            [banana]|\n",
            "|  Grace| 28|   TX|    2021-09-12|  90.1|                  []|\n",
            "|  Heidi| 42|   NY|    2019-08-01| 180.3|[banana, kiwi, ap...|\n",
            "|   Ivan| 60|   CA|    2017-06-25| 400.0|            [orange]|\n",
            "|   Judy| 33|   TX|    2020-12-01| 150.0|                NULL|\n",
            "+-------+---+-----+--------------+------+--------------------+\n",
            "\n",
            "+-------+---+-------+--------------+------+--------------------+\n",
            "|   Name|Age|  State|EnrollmentDate|Amount|               Items|\n",
            "+-------+---+-------+--------------+------+--------------------+\n",
            "|  Alice| 30|     NY|    2020-01-15| 100.5|     [apple, banana]|\n",
            "|    Bob| 45|     CA|    2019-03-20|250.75|     [orange, grape]|\n",
            "|Charlie| 25|     TX|    2021-07-01|  75.2|       [apple, kiwi]|\n",
            "|  David| 50|     NY|    2018-11-10| 300.0|            [banana]|\n",
            "|    Eve|  1|     CA|    2022-02-28| 120.0|      [grape, melon]|\n",
            "|  Frank| 35|Unknown|    2020-05-05|   0.0|     [apple, orange]|\n",
            "|  Grace| 28|     TX|    2021-09-12|  90.1|                  []|\n",
            "|  Heidi| 42|     NY|    2019-08-01| 180.3|[banana, kiwi, ap...|\n",
            "|   Ivan| 60|     CA|    2017-06-25| 400.0|            [orange]|\n",
            "|   Judy| 33|     TX|    2020-12-01| 150.0|                NULL|\n",
            "+-------+---+-------+--------------+------+--------------------+\n",
            "\n",
            "+-------+---------+------------+-----------+------------+-----------+\n",
            "|   Name|IsAgeNull|IsAgeNotNull|IsStateNull|IsAmountNull|IsAmountNaN|\n",
            "+-------+---------+------------+-----------+------------+-----------+\n",
            "|  Alice|    false|        true|      false|       false|      false|\n",
            "|    Bob|    false|        true|      false|       false|      false|\n",
            "|Charlie|    false|        true|      false|       false|      false|\n",
            "|  David|    false|        true|      false|       false|      false|\n",
            "|    Eve|     true|       false|      false|       false|      false|\n",
            "|  Frank|    false|        true|       true|        true|      false|\n",
            "|  Grace|    false|        true|      false|       false|      false|\n",
            "|  Heidi|    false|        true|      false|       false|      false|\n",
            "|   Ivan|    false|        true|      false|       false|      false|\n",
            "|   Judy|    false|        true|      false|       false|      false|\n",
            "+-------+---------+------------+-----------+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkoBZyM0Zu41",
        "outputId": "1b9e331c-c2bb-4929-e1c9-1b2ee75301cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+--------------+------+--------------------+\n",
            "|   Name| Age|State|EnrollmentDate|Amount|               Items|\n",
            "+-------+----+-----+--------------+------+--------------------+\n",
            "|  Alice|  30|   NY|    2020-01-15| 100.5|     [apple, banana]|\n",
            "|    Bob|  45|   CA|    2019-03-20|250.75|     [orange, grape]|\n",
            "|Charlie|  25|   TX|    2021-07-01|  75.2|       [apple, kiwi]|\n",
            "|  David|  50|   NY|    2018-11-10| 300.0|            [banana]|\n",
            "|    Eve|NULL|   CA|    2022-02-28| 120.0|      [grape, melon]|\n",
            "|  Frank|  35| NULL|    2020-05-05|  NULL|     [apple, orange]|\n",
            "|  Grace|  28|   TX|    2021-09-12|  90.1|                  []|\n",
            "|  Heidi|  42|   NY|    2019-08-01| 180.3|[banana, kiwi, ap...|\n",
            "|   Ivan|  60|   CA|    2017-06-25| 400.0|            [orange]|\n",
            "|   Judy|  33|   TX|    2020-12-01| 150.0|                NULL|\n",
            "+-------+----+-----+--------------+------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explode: Creates a new row for each element in an array column.\n",
        "# If the array is null or empty, the row is not generated.\n",
        "df_exploded = df.select(\"Name\", explode(\"Items\").alias(\"Item\"))\n",
        "print(\"DataFrame after exploding 'Items' array:\")\n",
        "df_exploded.show(5)\n",
        "\n",
        "# array_contains: Checks if an array column contains a specific value.\n",
        "df_contains_apple = df.withColumn(\"HasApple\", array_contains(col(\"Items\"), \"apple\"))\n",
        "print(\"DataFrame with 'HasApple' column:\")\n",
        "df_contains_apple.show(5)\n",
        "\n",
        "# size: Returns the size of an array or map.\n",
        "df_item_count = df.withColumn(\"ItemCount\", size(col(\"Items\")))\n",
        "print(\"DataFrame with 'ItemCount' column (size of Items array):\")\n",
        "df_item_count.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RnQt6mEbSlw",
        "outputId": "7371e6e6-90cf-41d2-e1a4-2370f4504e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame after exploding 'Items' array:\n",
            "+-------+------+\n",
            "|   Name|  Item|\n",
            "+-------+------+\n",
            "|  Alice| apple|\n",
            "|  Alice|banana|\n",
            "|    Bob|orange|\n",
            "|    Bob| grape|\n",
            "|Charlie| apple|\n",
            "+-------+------+\n",
            "only showing top 5 rows\n",
            "\n",
            "DataFrame with 'HasApple' column:\n",
            "+-------+----+-----+--------------+------+---------------+--------+\n",
            "|   Name| Age|State|EnrollmentDate|Amount|          Items|HasApple|\n",
            "+-------+----+-----+--------------+------+---------------+--------+\n",
            "|  Alice|  30|   NY|    2020-01-15| 100.5|[apple, banana]|    true|\n",
            "|    Bob|  45|   CA|    2019-03-20|250.75|[orange, grape]|   false|\n",
            "|Charlie|  25|   TX|    2021-07-01|  75.2|  [apple, kiwi]|    true|\n",
            "|  David|  50|   NY|    2018-11-10| 300.0|       [banana]|   false|\n",
            "|    Eve|NULL|   CA|    2022-02-28| 120.0| [grape, melon]|   false|\n",
            "+-------+----+-----+--------------+------+---------------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "DataFrame with 'ItemCount' column (size of Items array):\n",
            "+-------+----+-----+--------------+------+---------------+---------+\n",
            "|   Name| Age|State|EnrollmentDate|Amount|          Items|ItemCount|\n",
            "+-------+----+-----+--------------+------+---------------+---------+\n",
            "|  Alice|  30|   NY|    2020-01-15| 100.5|[apple, banana]|        2|\n",
            "|    Bob|  45|   CA|    2019-03-20|250.75|[orange, grape]|        2|\n",
            "|Charlie|  25|   TX|    2021-07-01|  75.2|  [apple, kiwi]|        2|\n",
            "|  David|  50|   NY|    2018-11-10| 300.0|       [banana]|        1|\n",
            "|    Eve|NULL|   CA|    2022-02-28| 120.0| [grape, melon]|        2|\n",
            "+-------+----+-----+--------------+------+---------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 12. Math Functions ---\n",
        "print(\"\\n--- Math Functions ---\")\n",
        "math_df = products_df.select(\n",
        "    col(\"product\"),\n",
        "    col(\"price\"),\n",
        "    round(col(\"price\"), 0).alias(\"Price_Rounded\"),\n",
        "    ceil(col(\"price\")).alias(\"Price_Ceil\"),\n",
        "    floor(col(\"price\")).alias(\"Price_Floor\"),\n",
        "    sqrt(col(\"price\")).alias(\"Price_Sqrt\"),\n",
        "    pow(col(\"price\"), 2).alias(\"Price_Squared\")\n",
        ")\n",
        "math_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daiW_vgybfVg",
        "outputId": "acbd8147-f6d8-49ed-e314-76e61666db15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Math Functions ---\n",
            "+--------+------+-------------+----------+-----------+------------------+-------------+\n",
            "| product| price|Price_Rounded|Price_Ceil|Price_Floor|        Price_Sqrt|Price_Squared|\n",
            "+--------+------+-------------+----------+-----------+------------------+-------------+\n",
            "|  Laptop|1200.0|       1200.0|      1200|       1200| 34.64101615137755|    1440000.0|\n",
            "|   Mouse|  25.5|         26.0|        26|         25| 5.049752469181039|       650.25|\n",
            "|Keyboard|  75.0|         75.0|        75|         75| 8.660254037844387|       5625.0|\n",
            "| Monitor| 300.0|        300.0|       300|        300|17.320508075688775|      90000.0|\n",
            "|  Webcam|  50.0|         50.0|        50|         50|7.0710678118654755|       2500.0|\n",
            "+--------+------+-------------+----------+-----------+------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Window Functions ---\n",
        "\n",
        "#window_spec = Window.partitionBy(\"State\").orderBy(\"Age\")\n",
        "\n",
        "# Add row number, rank, dense rank, and NTILE\n",
        "df_window = df.withColumn(\"row_number\", row_number().over(Window.partitionBy(\"State\").orderBy(\"Age\"))) \\\n",
        "              .withColumn(\"rank\", rank().over(Window.partitionBy(\"State\").orderBy(\"Age\"))) \\\n",
        "              .withColumn(\"dense_rank\", dense_rank().over(Window.partitionBy(\"State\").orderBy(\"Age\"))) \\\n",
        "              .withColumn(\"ntile_2\", ntile(2).over(Window.partitionBy(\"State\").orderBy(\"Age\")))\n",
        "df_window.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIJgHlgRchp1",
        "outputId": "1b97d85f-6e93-4e21-9284-a9478cbd90bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+--------------+------+--------------------+----------+----+----------+-------+\n",
            "|   Name| Age|State|EnrollmentDate|Amount|               Items|row_number|rank|dense_rank|ntile_2|\n",
            "+-------+----+-----+--------------+------+--------------------+----------+----+----------+-------+\n",
            "|  Frank|  35| NULL|    2020-05-05|  NULL|     [apple, orange]|         1|   1|         1|      1|\n",
            "|    Eve|NULL|   CA|    2022-02-28| 120.0|      [grape, melon]|         1|   1|         1|      1|\n",
            "|    Bob|  45|   CA|    2019-03-20|250.75|     [orange, grape]|         2|   2|         2|      1|\n",
            "|   Ivan|  60|   CA|    2017-06-25| 400.0|            [orange]|         3|   3|         3|      2|\n",
            "|  Alice|  30|   NY|    2020-01-15| 100.5|     [apple, banana]|         1|   1|         1|      1|\n",
            "|  Heidi|  42|   NY|    2019-08-01| 180.3|[banana, kiwi, ap...|         2|   2|         2|      1|\n",
            "|  David|  50|   NY|    2018-11-10| 300.0|            [banana]|         3|   3|         3|      2|\n",
            "|Charlie|  25|   TX|    2021-07-01|  75.2|       [apple, kiwi]|         1|   1|         1|      1|\n",
            "|  Grace|  28|   TX|    2021-09-12|  90.1|                  []|         2|   2|         2|      1|\n",
            "|   Judy|  33|   TX|    2020-12-01| 150.0|                NULL|         3|   3|         3|      2|\n",
            "+-------+----+-----+--------------+------+--------------------+----------+----+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lead: Accesses a value from a subsequent row.\n",
        "# Lag: Accesses a value from a previous row.\n",
        "df_window = df_window.withColumn(\"next_enrollment_date\", lead(col(\"EnrollmentDate\"), 1).over(Window.partitionBy(\"State\").orderBy(\"EnrollmentDate\"))) \\\n",
        "                     .withColumn(\"prev_enrollment_amount\", lag(col(\"Amount\"), 1).over(Window.partitionBy(\"State\").orderBy(\"EnrollmentDate\")))\n",
        "\n",
        "print(\"DataFrame with Window Functions:\")\n",
        "df_window.select(\"Name\", \"Age\", \"State\", \"EnrollmentDate\", \"Amount\",\n",
        "                 \"row_number\", \"rank\", \"dense_rank\", \"ntile_2\",\n",
        "                 \"next_enrollment_date\", \"prev_enrollment_amount\").orderBy(\"State\", \"Age\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JD6HBFbdDNh",
        "outputId": "5760dd63-bea4-43f1-a46f-cdd97140c39d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with Window Functions:\n",
            "+-------+----+-----+--------------+------+----------+----+----------+-------+--------------------+----------------------+\n",
            "|   Name| Age|State|EnrollmentDate|Amount|row_number|rank|dense_rank|ntile_2|next_enrollment_date|prev_enrollment_amount|\n",
            "+-------+----+-----+--------------+------+----------+----+----------+-------+--------------------+----------------------+\n",
            "|  Frank|  35| NULL|    2020-05-05|  NULL|         1|   1|         1|      1|                NULL|                  NULL|\n",
            "|    Eve|NULL|   CA|    2022-02-28| 120.0|         1|   1|         1|      1|                NULL|                250.75|\n",
            "|    Bob|  45|   CA|    2019-03-20|250.75|         2|   2|         2|      1|          2022-02-28|                 400.0|\n",
            "|   Ivan|  60|   CA|    2017-06-25| 400.0|         3|   3|         3|      2|          2019-03-20|                  NULL|\n",
            "|  Alice|  30|   NY|    2020-01-15| 100.5|         1|   1|         1|      1|                NULL|                 180.3|\n",
            "|  Heidi|  42|   NY|    2019-08-01| 180.3|         2|   2|         2|      1|          2020-01-15|                 300.0|\n",
            "|  David|  50|   NY|    2018-11-10| 300.0|         3|   3|         3|      2|          2019-08-01|                  NULL|\n",
            "|Charlie|  25|   TX|    2021-07-01|  75.2|         1|   1|         1|      1|          2021-09-12|                 150.0|\n",
            "|  Grace|  28|   TX|    2021-09-12|  90.1|         2|   2|         2|      1|                NULL|                  75.2|\n",
            "|   Judy|  33|   TX|    2020-12-01| 150.0|         3|   3|         3|      2|          2021-07-01|                  NULL|\n",
            "+-------+----+-----+--------------+------+----------+----+----------+-------+--------------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "format_df = df.select(\n",
        "    col(\"Name\"),\n",
        "    col(\"Amount\"),\n",
        "    format_number(col(\"Amount\"), 2).alias(\"Amount_Formatted\"), # Format to 2 decimal places\n",
        "    format_string(\"Name: %s, Age: %d\", col(\"Name\"), col(\"Age\")).alias(\"Formatted_String\")\n",
        ")\n",
        "format_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcwaIc-jeRsD",
        "outputId": "7185711d-1af2-4118-f433-b9c90214636d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------+----------------+--------------------+\n",
            "|   Name|Amount|Amount_Formatted|    Formatted_String|\n",
            "+-------+------+----------------+--------------------+\n",
            "|  Alice| 100.5|          100.50|Name: Alice, Age: 30|\n",
            "|    Bob|250.75|          250.75|  Name: Bob, Age: 45|\n",
            "|Charlie|  75.2|           75.20|Name: Charlie, Ag...|\n",
            "|  David| 300.0|          300.00|Name: David, Age: 50|\n",
            "|    Eve| 120.0|          120.00|Name: Eve, Age: null|\n",
            "|  Frank|  NULL|            NULL|Name: Frank, Age: 35|\n",
            "|  Grace|  90.1|           90.10|Name: Grace, Age: 28|\n",
            "|  Heidi| 180.3|          180.30|Name: Heidi, Age: 42|\n",
            "|   Ivan| 400.0|          400.00| Name: Ivan, Age: 60|\n",
            "|   Judy| 150.0|          150.00| Name: Judy, Age: 33|\n",
            "+-------+------+----------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# User Defined Functions (UDF) ---\n",
        "\n",
        "# Define a Python function\n",
        "def get_age_category(age):\n",
        "    if age is None:\n",
        "        return \"Unknown\"\n",
        "    elif age < 18:\n",
        "        return \"Minor\"\n",
        "    elif 18 <= age < 60:\n",
        "        return \"Adult\"\n",
        "    else:\n",
        "        return \"Senior\"\n",
        "\n",
        "# Register the Python function as a UDF using the decorator syntax (@udf)\n",
        "# This is syntactic sugar for udf(get_age_category, StringType())\n",
        "\n",
        "@udf(StringType())\n",
        "def get_age_category_udf(age):\n",
        "    return get_age_category(age)\n",
        "\n",
        "# Apply the UDF to the DataFrame\n",
        "df_with_udf = df.withColumn(\"AgeCategory\", get_age_category_udf(col(\"Age\")))\n",
        "print(\"DataFrame with UDF 'AgeCategory':\")\n",
        "df_with_udf.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-ExZa1UekXr",
        "outputId": "2297f77e-feb6-4867-a447-576db34194a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with UDF 'AgeCategory':\n",
            "+-------+----+-----+--------------+------+--------------------+-----------+\n",
            "|   Name| Age|State|EnrollmentDate|Amount|               Items|AgeCategory|\n",
            "+-------+----+-----+--------------+------+--------------------+-----------+\n",
            "|  Alice|  30|   NY|    2020-01-15| 100.5|     [apple, banana]|      Adult|\n",
            "|    Bob|  45|   CA|    2019-03-20|250.75|     [orange, grape]|      Adult|\n",
            "|Charlie|  25|   TX|    2021-07-01|  75.2|       [apple, kiwi]|      Adult|\n",
            "|  David|  50|   NY|    2018-11-10| 300.0|            [banana]|      Adult|\n",
            "|    Eve|NULL|   CA|    2022-02-28| 120.0|      [grape, melon]|    Unknown|\n",
            "|  Frank|  35| NULL|    2020-05-05|  NULL|     [apple, orange]|      Adult|\n",
            "|  Grace|  28|   TX|    2021-09-12|  90.1|                  []|      Adult|\n",
            "|  Heidi|  42|   NY|    2019-08-01| 180.3|[banana, kiwi, ap...|      Adult|\n",
            "|   Ivan|  60|   CA|    2017-06-25| 400.0|            [orange]|     Senior|\n",
            "|   Judy|  33|   TX|    2020-12-01| 150.0|                NULL|      Adult|\n",
            "+-------+----+-----+--------------+------+--------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another UDF example using the explicit udf() function\n",
        "def calculate_discount(amount, is_senior):\n",
        "    if amount is None:\n",
        "        return None\n",
        "    if is_senior == \"Senior\":\n",
        "        return amount * 0.90 # 10% discount for seniors\n",
        "    return amount\n",
        "\n",
        "# Register the UDF with appropriate return type\n",
        "calculate_discount_udf = udf(calculate_discount, DoubleType())\n",
        "\n",
        "df_with_udf_discount = df_with_udf.withColumn(\n",
        "    \"DiscountedAmount\",\n",
        "    calculate_discount_udf(col(\"Amount\"), col(\"AgeCategory\"))\n",
        ")\n",
        "print(\"DataFrame with UDF 'DiscountedAmount':\")\n",
        "df_with_udf_discount.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtrqXFlxfhpY",
        "outputId": "e54b0702-9db8-4c66-8c7e-721cba2a83e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with UDF 'DiscountedAmount':\n",
            "+-------+----+-----+--------------+------+--------------------+-----------+----------------+\n",
            "|   Name| Age|State|EnrollmentDate|Amount|               Items|AgeCategory|DiscountedAmount|\n",
            "+-------+----+-----+--------------+------+--------------------+-----------+----------------+\n",
            "|  Alice|  30|   NY|    2020-01-15| 100.5|     [apple, banana]|      Adult|           100.5|\n",
            "|    Bob|  45|   CA|    2019-03-20|250.75|     [orange, grape]|      Adult|          250.75|\n",
            "|Charlie|  25|   TX|    2021-07-01|  75.2|       [apple, kiwi]|      Adult|            75.2|\n",
            "|  David|  50|   NY|    2018-11-10| 300.0|            [banana]|      Adult|           300.0|\n",
            "|    Eve|NULL|   CA|    2022-02-28| 120.0|      [grape, melon]|    Unknown|           120.0|\n",
            "|  Frank|  35| NULL|    2020-05-05|  NULL|     [apple, orange]|      Adult|            NULL|\n",
            "|  Grace|  28|   TX|    2021-09-12|  90.1|                  []|      Adult|            90.1|\n",
            "|  Heidi|  42|   NY|    2019-08-01| 180.3|[banana, kiwi, ap...|      Adult|           180.3|\n",
            "|   Ivan|  60|   CA|    2017-06-25| 400.0|            [orange]|     Senior|           360.0|\n",
            "|   Judy|  33|   TX|    2020-12-01| 150.0|                NULL|      Adult|           150.0|\n",
            "+-------+----+-----+--------------+------+--------------------+-----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add some configurations (example: setting shuffle partitions)\n",
        "# Spark.sql.shuffle.partitions: Controls the number of partitions used in shuffle operations.\n",
        "# A higher number can reduce partition size but increase overhead.\n",
        "spark_builder.config(\"spark.sql.shuffle.partitions\", \"8\") # Increased for demonstration\n",
        "spark_builder.config(\"spark.driver.memory\", \"2g\")"
      ],
      "metadata": {
        "id": "5W0o8mqwfwfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
        "#To set configurations\n",
        "#spark_builder.conf.set(\"spark.sql.shuffle.partitions\", \"8\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ve-FFrlAgT0c",
        "outputId": "0388efd2-15fc-4f8c-a2c2-d7228593af20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'200'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.get(\"spark.driver.memory\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "uaWF-aG-gmfM",
        "outputId": "5bb11cbe-2280-4755-c3b5-3299feec53f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o635.get.\n: org.apache.spark.SparkNoSuchElementException: [SQL_CONF_NOT_FOUND] The SQL config \"spark.driver.memory\" cannot be found. Please verify that the config exists.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sqlConfigNotFoundError(QueryExecutionErrors.scala:1984)\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:5274)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:5274)\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:81)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-46-3887352599.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.driver.memory\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/conf.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0m_NoValue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o635.get.\n: org.apache.spark.SparkNoSuchElementException: [SQL_CONF_NOT_FOUND] The SQL config \"spark.driver.memory\" cannot be found. Please verify that the config exists.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.sqlConfigNotFoundError(QueryExecutionErrors.scala:1984)\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:5274)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:5274)\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:81)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.getConf().get(\"spark.driver.memory\")"
      ],
      "metadata": {
        "id": "JLD9LdWQg5dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample DataFrame\n",
        "data = [\n",
        "    (\"Alice\", 25, \"NY\", 50000.0, True),\n",
        "    (\"Bob\", 30, \"CA\", 60000.5, False),\n",
        "    (\"Charlie\", 35, \"TX\", 70000.0, True),\n",
        "    (\"David\", 28, \"NY\", 55000.0, False)\n",
        "]\n",
        "columns = [\"Name\", \"Age\", \"State\", \"Salary\", \"IsActive\"]\n",
        "sample_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "print(\"--- Original DataFrame to be saved ---\")\n",
        "sample_df.show()\n",
        "sample_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofTlZ4lvhPVp",
        "outputId": "be0ebe60-ac41-4052-bdfe-da5a63dfaf4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Original DataFrame to be saved ---\n",
            "+-------+---+-----+-------+--------+\n",
            "|   Name|Age|State| Salary|IsActive|\n",
            "+-------+---+-----+-------+--------+\n",
            "|  Alice| 25|   NY|50000.0|    true|\n",
            "|    Bob| 30|   CA|60000.5|   false|\n",
            "|Charlie| 35|   TX|70000.0|    true|\n",
            "|  David| 28|   NY|55000.0|   false|\n",
            "+-------+---+-----+-------+--------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Salary: double (nullable = true)\n",
            " |-- IsActive: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths for saving\n",
        "parquet_path = \"temp_data/sample.parquet\"\n",
        "csv_path = \"temp_data/sample.csv\"\n",
        "json_path = \"temp_data/sample.json\"\n",
        "orc_path = \"temp_data/sample.orc\"\n",
        "\n",
        "# 3. Save the DataFrame in different formats (for loading demonstrations)\n",
        "# Note: coalesce(1) is used to create a single file for simplicity,\n",
        "# but it's not recommended for large-scale production writes.\n",
        "sample_df.write.mode(\"overwrite\").format(\"parquet\").save(parquet_path)\n",
        "sample_df.write.mode(\"overwrite\").format(\"csv\").option(\"header\", \"true\").save(csv_path)\n",
        "sample_df.write.mode(\"overwrite\").format(\"json\").save(json_path)\n",
        "sample_df.write.mode(\"overwrite\").format(\"orc\").save(orc_path)"
      ],
      "metadata": {
        "id": "JuCLRnBikaIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Parquet\n",
        "print(\"\\n--- Loading Parquet Data ---\")\n",
        "df_parquet = spark.read.load( path=parquet_path, format=\"parquet\" )\n",
        "df_parquet.show()\n",
        "df_parquet.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoqbpTcRkgEa",
        "outputId": "431947d1-e460-4bfe-8d3c-1781cfe60015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading Parquet Data ---\n",
            "+-------+---+-----+-------+--------+\n",
            "|   Name|Age|State| Salary|IsActive|\n",
            "+-------+---+-----+-------+--------+\n",
            "|Charlie| 35|   TX|70000.0|    true|\n",
            "|  David| 28|   NY|55000.0|   false|\n",
            "|  Alice| 25|   NY|50000.0|    true|\n",
            "|    Bob| 30|   CA|60000.5|   false|\n",
            "+-------+---+-----+-------+--------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Salary: double (nullable = true)\n",
            " |-- IsActive: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Loading CSV Data ---\")\n",
        "df_csv = spark.read.load(\n",
        "    path=csv_path,\n",
        "    format=\"csv\",\n",
        "    header=\"true\",       # Specify that the first row is a header\n",
        "    inferSchema=\"true\"   # Infer data types automatically\n",
        "    # Or, to manually specify schema for better control:\n",
        "    # schema=your_manual_schema\n",
        ")\n",
        "df_csv.show()\n",
        "df_csv.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLgQfFaxkpNU",
        "outputId": "02da16c6-1084-452b-857c-fd1875fb109e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading CSV Data ---\n",
            "+-------+---+-----+-------+--------+\n",
            "|   Name|Age|State| Salary|IsActive|\n",
            "+-------+---+-----+-------+--------+\n",
            "|Charlie| 35|   TX|70000.0|    true|\n",
            "|  David| 28|   NY|55000.0|   false|\n",
            "|  Alice| 25|   NY|50000.0|    true|\n",
            "|    Bob| 30|   CA|60000.5|   false|\n",
            "+-------+---+-----+-------+--------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Salary: double (nullable = true)\n",
            " |-- IsActive: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Loading JSON Data ---\")\n",
        "df_json = spark.read.load(\n",
        "    path=json_path,\n",
        "    format=\"json\",\n",
        "    inferSchema=\"true\" # inferSchema can be useful for JSON\n",
        ")\n",
        "df_json.show()\n",
        "df_json.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuVAZGFKk68t",
        "outputId": "605b0e37-0b31-4a25-9304-f5b1038a1137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading JSON Data ---\n",
            "+---+--------+-------+-------+-----+\n",
            "|Age|IsActive|   Name| Salary|State|\n",
            "+---+--------+-------+-------+-----+\n",
            "| 35|    true|Charlie|70000.0|   TX|\n",
            "| 28|   false|  David|55000.0|   NY|\n",
            "| 25|    true|  Alice|50000.0|   NY|\n",
            "| 30|   false|    Bob|60000.5|   CA|\n",
            "+---+--------+-------+-------+-----+\n",
            "\n",
            "root\n",
            " |-- Age: long (nullable = true)\n",
            " |-- IsActive: boolean (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Salary: double (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Loading ORC Data ---\")\n",
        "df_orc = spark.read.load(\n",
        "    path=orc_path,\n",
        "    format=\"orc\"\n",
        ")\n",
        "df_orc.show()\n",
        "df_orc.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olwfYoHklBs9",
        "outputId": "4a65553f-2cb2-47e6-f7f7-be32801ff7c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading ORC Data ---\n",
            "+-------+---+-----+-------+--------+\n",
            "|   Name|Age|State| Salary|IsActive|\n",
            "+-------+---+-----+-------+--------+\n",
            "|Charlie| 35|   TX|70000.0|    true|\n",
            "|  David| 28|   NY|55000.0|   false|\n",
            "|  Alice| 25|   NY|50000.0|    true|\n",
            "|    Bob| 30|   CA|60000.5|   false|\n",
            "+-------+---+-----+-------+--------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Salary: double (nullable = true)\n",
            " |-- IsActive: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Loading CSV Data using options() ---\")\n",
        "csv_options = {\n",
        "    \"header\": \"true\",\n",
        "    \"inferSchema\": \"true\",\n",
        "    \"delimiter\": \",\" # explicitly define delimiter if needed\n",
        "}\n",
        "df_csv_options = spark.read.load(\n",
        "    path=csv_path,\n",
        "    format=\"csv\",\n",
        "    **csv_options # Unpack the dictionary\n",
        ")\n",
        "df_csv_options.show()\n",
        "df_csv_options.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLH3i-5elFD0",
        "outputId": "25844d61-3e79-406c-9ccb-c592cc81955b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Loading CSV Data using options() ---\n",
            "+-------+---+-----+-------+--------+\n",
            "|   Name|Age|State| Salary|IsActive|\n",
            "+-------+---+-----+-------+--------+\n",
            "|Charlie| 35|   TX|70000.0|    true|\n",
            "|  David| 28|   NY|55000.0|   false|\n",
            "|  Alice| 25|   NY|50000.0|    true|\n",
            "|    Bob| 30|   CA|60000.5|   false|\n",
            "+-------+---+-----+-------+--------+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- State: string (nullable = true)\n",
            " |-- Salary: double (nullable = true)\n",
            " |-- IsActive: boolean (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Database Functions\n",
        "\n",
        "#To check the current Database\n",
        "spark.catalog.currentDatabase\n",
        "#To set the current Database\n",
        "spark.catalog.setCurrentDatabase('test2')\n",
        "#To set the list of Databases\n",
        "spark.catalog.listDatabases[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "BVQFVTc-lI9s",
        "outputId": "09334ed3-73c2-4a38-d659-b9ff56a6b9bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Catalog.currentDatabase of <pyspark.sql.catalog.Catalog object at 0x799bde5d4390>>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.catalog.Catalog.currentDatabase</b><br/>def currentDatabase() -&gt; str</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/sql/catalog.py</a>Returns the current default database in this session.\n",
              "\n",
              ".. versionadded:: 2.0.0\n",
              "\n",
              "Returns\n",
              "-------\n",
              "str\n",
              "    The current default database name.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; spark.catalog.currentDatabase()\n",
              "&#x27;default&#x27;</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 163);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Table Functions\n",
        "\n",
        "#To check the list of  columns\n",
        "spark.catalog.listcolumns\n",
        "#To check the list of tables in current database\n",
        "spark.catalog.listTables\n",
        "#To check the list of cache table\n",
        "spark.catalog.cacheTable\n",
        "#To check the list of un cache table\n",
        "spark.catalog.uncacheTable\n",
        "#To check the list of iscached\n",
        "spark.catalog.iscache\n",
        "#To check the list of clear cache\n",
        "spark.catalog.clearCache\n",
        "#To check the list of recoverPartitions\n",
        "spark.catalog.recoverPartitions\n",
        "#To check the list of refreshTable\n",
        "spark.catalog.refreshTable\n",
        "#To check the list of refreshpath\n",
        "spark.catalog.refreshpath"
      ],
      "metadata": {
        "id": "vIy13Do3n0Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View Functions\n",
        "# drop temporaryview\n",
        "# drop globaltemporaryview\n"
      ],
      "metadata": {
        "id": "itwnKiqXphG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Broadcasting Pros:\n",
        "  # 1. Eliminates Shuffle: The primary benefit is avoiding the network I/O and disk writes associated with shuffling the larger table.\n",
        "      # This is a huge performance gain.\n",
        "  # 2. Faster for Small-Large Joins: Ideal when one table is small enough to fit comfortably in executor memory.\n",
        "\n",
        "# Broadcasting Cons:\n",
        "  # 1. Memory Intensive: If the \"small\" table is too large, broadcasting it can cause OutOfMemoryError (OOM) on the driver or the executors.\n",
        "  # 2. Driver Bottleneck: The driver might struggle to collect and serialize a very large table for broadcasting.\n",
        "  # 3. joined_df = large_df.join(broadcast(small_df), \"key\", \"inner\")"
      ],
      "metadata": {
        "id": "uzmowZN_tSJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bucketing / Partitioning / Salting"
      ],
      "metadata": {
        "id": "VQDw07hjuB70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# repartition() vs coalesce()"
      ],
      "metadata": {
        "id": "-ELWH9N5uHPC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}